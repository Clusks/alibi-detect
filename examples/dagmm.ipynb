{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Reshape, InputLayer\n",
    "from tensorflow.keras.losses import mse\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "from odcd.datasets import fetch_kdd\n",
    "from odcd.utils.data import create_outlier_batch\n",
    "from odcd.utils.saving import save_od, load_od\n",
    "from odcd.utils.visualize import plot_instance_outlier, plot_feature_outlier_tabular\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "We only keep a number of continuous (18 out of 41) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494021, 18) (494021,)\n"
     ]
    }
   ],
   "source": [
    "kddcup = fetch_kdd(percent10=True)  # only load 10% of the dataset\n",
    "print(kddcup.data.shape, kddcup.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that a model is trained on *normal* instances of the dataset (not outliers) and standardization is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 18) (400000,)\n",
      "0.0% outliers\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "normal_batch = create_outlier_batch(kddcup.data, kddcup.target, n_samples=400000, perc_outlier=0)\n",
    "data, target = normal_batch.data.astype('float'), normal_batch.target\n",
    "print(data.shape, target.shape)\n",
    "print('{}% outliers'.format(100 * target.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, stdev = data.mean(axis=0), data.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batch of data with 10% outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 18) (100000,)\n",
      "10.0% outliers\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "outlier_batch = create_outlier_batch(kddcup.data, kddcup.target, n_samples=100000, perc_outlier=10)\n",
    "X_train, y_train = outlier_batch.data.astype('float'), outlier_batch.target\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('{}% outliers'.format(100 * y_train.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - mean) / stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_euclidean_distance(x: tf.Tensor, y: tf.Tensor, axis: int = -1):\n",
    "    # TODO: make sure also works for higher dim enc eg for images\n",
    "    dist = tf.norm(x - y, axis=axis) / tf.norm(x, axis=axis)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class DAGMM(tf.keras.Model):\n",
    "    \"\"\"  Deep Autoencoding Gaussian Mixture Model.  \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder_net: tf.keras.Sequential,\n",
    "                 decoder_net: tf.keras.Sequential,\n",
    "                 gmm_density_net: tf.keras.Sequential,\n",
    "                 n_gmm: int = 5,\n",
    "                 latent_dim: int = 10,  # TODO: infer from encoder_net\n",
    "                 name: str = 'dagmm') -> None:\n",
    "        super(DAGMM, self).__init__(name=name)\n",
    "        self.encoder = encoder_net\n",
    "        self.decoder = decoder_net\n",
    "        self.gmm_density = gmm_density_net\n",
    "        self.n_gmm = n_gmm\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        enc = self.encoder(x)\n",
    "        x_recon = self.decoder(enc)\n",
    "        if len(x.shape) > 2:\n",
    "            x = Flatten()(x)\n",
    "            x_recon = Flatten()(x_recon)\n",
    "        if len(enc.shape) > 2:\n",
    "            enc = Flatten()(enc)\n",
    "        rec_cos = tf.keras.losses.cosine_similarity(x, x_recon, -1)\n",
    "        rec_euc = relative_euclidean_distance(x, x_recon, -1)\n",
    "        z = tf.concat([enc, rec_cos, rec_euc], -1)\n",
    "        gamma = self.gmm_density(z)\n",
    "        # TODO: check whether reshaping before returning is needed for x_recon\n",
    "        return enc, x_recon, z, gamma\n",
    "\n",
    "    def gmm_params(self,\n",
    "                   z: tf.Tensor,\n",
    "                   gamma: tf.Tensor) \\\n",
    "            -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "\n",
    "        # nb of samples in batch\n",
    "        N = gamma.shape[0]\n",
    "\n",
    "        # K\n",
    "        sum_gamma = K.sum(gamma, 0)\n",
    "\n",
    "        # K but should be [K, 1]?; check\n",
    "        phi = sum_gamma / N\n",
    "\n",
    "        # K x D (D = latent_dim)\n",
    "        mu = (K.sum(tf.expand_dims(gamma, -1) * tf.expand_dims(z, 1), 0)\n",
    "              / tf.expand_dims(sum_gamma, -1))\n",
    "\n",
    "        # N x K x D\n",
    "        z_mu = tf.expand_dims(z, 1) - tf.expand_dims(mu, 0)\n",
    "\n",
    "        # N x K x D x D\n",
    "        z_mu_outer = tf.expand_dims(z_mu, -1) * tf.expand_dims(z_mu, -2)\n",
    "\n",
    "        # K x D x D\n",
    "        cov = (K.sum(tf.expand_dims(tf.expand_dims(gamma, -1), -1) * z_mu_outer, 0)\n",
    "               / tf.expand_dims(tf.expand_dims(sum_gamma, -1), -1))\n",
    "\n",
    "        return phi, mu, cov\n",
    "\n",
    "    def gmm_energy(self,\n",
    "                   z: tf.Tensor,\n",
    "                   phi: tf.Tensor,\n",
    "                   mu: tf.Tensor,\n",
    "                   cov: tf.Tensor,\n",
    "                   return_mean: bool = True) \\\n",
    "            -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "\n",
    "        K, D, _ = tf.shape(cov)\n",
    "\n",
    "        # N x K x D\n",
    "        z_mu = tf.expand_dims(z, 1) - tf.expand_dims(mu, 0)\n",
    "\n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "        eps = tf.constant(1e-12)\n",
    "        for i in range(K):\n",
    "            cov_k = cov[i] + tf.eye(D) * eps\n",
    "            cov_k_inv = tf.expand_dims(tf.linalg.inv(cov_k), 0)\n",
    "            cov_inverse.append(cov_k_inv)\n",
    "            # use product of diagonals of LT from cholesky decomposition to compute determinant of matrix\n",
    "            det_cov_k = tf.expand_dims(tf.reduce_prod(tf.linalg.diag(tf.linalg.cholesky(cov_k * 2 * np.pi))), 0)\n",
    "            det_cov.append(det_cov_k)\n",
    "            cov_diag += K.sum(1 / tf.linalg.diag(cov_k))\n",
    "\n",
    "        # K x D x D\n",
    "        cov_inverse = tf.concat(cov_inverse, 0)\n",
    "\n",
    "        # K\n",
    "        det_cov = tf.concat(det_cov)\n",
    "\n",
    "        # N x K\n",
    "        exp_term_tmp = -.5 * K.sum(K.sum(tf.expand_dims(z_mu, -1) * tf.expand_dims(cov_inverse, 0), -2) * z_mu, -1)\n",
    "        # use logsumexp trick for stability\n",
    "        max_val = K.max(K.clip(exp_term_tmp, 0, 1e12), axis=1, keepdims=True)\n",
    "        exp_term = K.exp(exp_term_tmp - max_val)\n",
    "\n",
    "        # N\n",
    "        sample_energy = (- tf.squeeze(max_val) -\n",
    "                         tf.log(K.sum(tf.expand_dims(phi, 0) * exp_term /\n",
    "                                      tf.expand_dims(tf.sqrt(det_cov), 0), 1) + eps))\n",
    "\n",
    "        if return_mean:\n",
    "            sample_energy = tf.reduce_mean(sample_energy)\n",
    "\n",
    "        return sample_energy, cov_diag\n",
    "\n",
    "    def loss_fn(self,\n",
    "                x: tf.Tensor,\n",
    "                recon_x: tf.Tensor,\n",
    "                z: tf.Tensor,\n",
    "                gamma: tf.Tensor,\n",
    "                w_energy: float = .1,\n",
    "                w_cov_diag: float = .005) \\\n",
    "            -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        recon_error = K.mean((x - recon_x) ** 2)\n",
    "        phi, mu, cov = self.gmm_params(z, gamma)\n",
    "        sample_energy, cov_diag = self.gmm_energy(z, phi, mu, cov)\n",
    "        loss = recon_error + w_energy * sample_energy + w_cov_diag * cov_diag\n",
    "        return loss, sample_energy, recon_error, cov_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cdod] *",
   "language": "python",
   "name": "conda-env-cdod-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
